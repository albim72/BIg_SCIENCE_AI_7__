{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cn-QrNFKne5-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\",url,untar=True,cache_dir='.',cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset),'aclImdb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC4wIqgns26h",
        "outputId": "8a65e374-53d5-4820-a7da-3731b3852d7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 20s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQU6OhoMuxHy",
        "outputId": "c2d0f308-bb2a-464c-d72e-0de9a9a72b26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'README', 'imdb.vocab', 'imdbEr.txt', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(dataset_dir,'train')\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oekly-VTu2F-",
        "outputId": "b02f0957-44f0-4308-88e4-fa7bb4adef00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_unsup.txt',\n",
              " 'neg',\n",
              " 'unsup',\n",
              " 'urls_pos.txt',\n",
              " 'unsupBow.feat',\n",
              " 'pos',\n",
              " 'labeledBow.feat',\n",
              " 'urls_neg.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_dir = os.path.join(train_dir,'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "9RN2RX1awTVq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9elEQuzx7uD",
        "outputId": "8b8570a1-4ee2-49c1-9e56-7b68cc414e05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_unsup.txt',\n",
              " 'neg',\n",
              " 'urls_pos.txt',\n",
              " 'unsupBow.feat',\n",
              " 'pos',\n",
              " 'labeledBow.feat',\n",
              " 'urls_neg.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_file = os.path.join(train_dir,'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IlJScwwx-_G",
        "outputId": "125555da-ccf8-4130-f902-d826b5cb2cce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seed=42\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koKUi8Z7yVTG",
        "outputId": "e4b9a7be-75d2-4cd3-a6aa-6058bffa21b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(\"Review\", text_batch.numpy()[i])\n",
        "        print(\"Label\", label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdOTOAyX0u_0",
        "outputId": "3577a375-8e9b-4700-e3ae-886fe27b4b55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "Label 0\n",
            "Review b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
            "Label 0\n",
            "Review b'Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the\"High Fat Diet\" and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\\'s and that is what this is, a Great Documentary.....'\n",
            "Label 1\n",
            "Review b\"It's boggles the mind how this movie was nominated for seven Oscars and won one. Not because it's abysmal or because given the collective credentials of the creative team behind it really ought to deserve them but because in every category it was nominated Prizzi's Honor disappoints. Some would argue that old Hollywood pioneer John Huston had lost it by this point in his career but I don't buy it. Only the previous year he signed the superb UNDER THE VOLCANO, a dark character study set in Mexico, that ranks among the finest he ever did. Prizzi's Honor on the other hand, a film loaded with star power, good intentions and a decent script, proves to be a major letdown.<br /><br />The overall tone and plot of a gangster falling in love with a female hit-man prefigures the quirky crimedies that caught Hollywood by storm in the early 90's but the script is too convoluted for its own sake, the motivations are off and on the whole the story seems unsure of what exactly it's trying to be: a romantic comedy, a crime drama, a gangster saga etc. Jack Nicholson (doing a Brooklyn accent that works perfectly for De Niro but sounds unconvincing coming from Jack) and Kathleen Turner in the leading roles seem to be in paycheck mode, just going through the motions almost sleepwalking their way through some parts. Anjelica Huston on the other hand fares better but her performance is sabotaged by her character's motivations: she starts out the victim of her bigot father's disdain, she proves to be supportive to her ex-husband, then becomes a vindictive bitch that wants his head on a plate.<br /><br />The colours of the movie have a washed-up quality like it was made in the early 70's and Huston's direction is as uninteresting as everything else. There's promise behind the story and perhaps in the hands of a director hungry to be recognized it could've been morphed to something better but what's left looks like a film nobody was really interested in making.\"\n",
            "Label 0\n",
            "Review b'The concept of the legal gray area in Love Crimes contributes to about 10% of the movie\\'s appeal; the other 90% can be attributed to it\\'s flagrant bad-ness. To say that Sean Young\\'s performance as a so-called district attorney is wooden is a gross understatement. With her bland suits and superfluous hair gel, Young does a decent job at convincing the audience of her devout hatred for men. Why else would she ask her only friend to pose as a prostitute just so she can arrest cops who try to pick up on them? This hatred is also the only reason why she relentlessly pursues a perverted photographer who gives women a consensual thrill and the driving force behind this crappy movie. Watching Young go from frigid to full-frontal nudity does little to raise interest, but the temper tantrum she throws standing next to a fire by a lake does. Watching her rant and rave about her self-loathing and sexual frustration makes Love Crimes worth the rental fee, but it\\'s all downhill to and from there. Despite her urge to bring Patrick Bergin\\'s character to justice, her policing skills completely escape her in the throes of her own tired lust and passion. Patrick Bergin does a decent enough job as a slimy sociopath; if it worked in Sleeping With the Enemy it sure as hell can work in this. But I can\\'t help but wonder if the noticeable lack of energy Young brings to the film conflicts with his sliminess. I\\'m guessing it does and the result is a \"thriller\" with thrills that are thoroughly bad and yet comedic.'\n",
            "Label 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label 0  to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1  to\", raw_train_ds.class_names[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB67tZqO2mat",
        "outputId": "68bebce0-8cc8-4c19-8017-eec4281dcdd1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0  to neg\n",
            "Label 1  to pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtBcBY263Dri",
        "outputId": "f31a1756-a1cf-4395-800b-f2258e1dcf10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2R674K33ZCt",
        "outputId": "d566d192-f0a1-4442-8f0e-d9c8f99a1c99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#standardyzacja danych tekstowych\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase,'<br />',b' ')\n",
        "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')"
      ],
      "metadata": {
        "id": "EiG293FT38NG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wektoryzacja tekstu\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "metadata": {
        "id": "wWl_GKR_42SO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adaptacja warstwy wektoryzacji do tekstu(bez etykiety)\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "metadata": {
        "id": "0-9nQ8Lh5TUU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_batch,label_batch = next(iter(raw_test_ds))\n",
        "first_review,first_label = text_batch[0],label_batch[0]\n",
        "print(\"Review\",first_review)\n",
        "print(\"Label\",raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\",vectorize_layer(first_review))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1EwmIKQ6Dqo",
        "outputId": "81e4cdef-b474-4813-a9e2-d0067e972b89"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review tf.Tensor(b\"Granted, HOTD 2 is better than the Uwe Boll crapfest that was the first one, but thats like saying drowning is better than being chopped alive. OK OK, I'm being a little bit harsh with this one, its just that Video Game adaptations of Zombie movies always leave a bad taste in my mouth. Resident Evil was alright, but its sequels are pure rubbish. The first HOTD was entertaining crap more than anything else; Uwe Boll saw to that. And HOTD 2 was...better? The entertainment value was there, the hot chicks were there, the gore was there, but... this is it? The script, even though subpar, was better than the first one. But in the script dwells horribly written characters, stupid action sequences, clich\\xc3\\xa9s, and an ending that drags on and on reaching a level of ridiculousness which would make the Bollmeister very, very proud. But, it could've been worse right?<br /><br />HOTD 2 stars Sig Haig (Yes, Sid Haig) as an insane doctor looking for a cure to death. As you may guess, it all goes to sh*t. A nearby College Campus seems to be the center for the Zombie (or Hypersapien?) epidemic, and as you may expect, a group of Commandos and two Zombie scientists try to save the world by going there and neutralizing the problem. Their main goal? No, it ain't blowing sh*t up. Instead, they have to get a sample of zombie blood, so they can create a cure to the deadly virus. Between this and that lays bland character interactions, an insufferable amount of clich\\xc3\\xa9s (but hey, at least the black guy isn't the first one to get killed), and decent looking Zombies. <br /><br />The acting is passable. Who would've thought that Sticky Fingaz was a passable actor? Not me. Emmanuelle Vaugier and Ed Quinn shine somewhat as the leads. And the supporting cast was passable too. It was all passable. <br /><br />The make up effects on the Zombies were alright, it could've been worse like in the first HOTD: just guys running around in dusty mummy suits. The gore was alright too. Cheap scares and even cheaper tension are almost non-existent in this movie, which is a good thing. The action sequences were stupid as f*ck, but at least there wasn't any sword fighting zombies or that ridiculous slow motion bullet effect. <br /><br />Overall, this movie will appeal only to Zombie enthusiasts, fans of the arcade games and for someone who wants to rent a cheap Horror movie for the weekend. Apparently there's going to be a third one as hinted by the ending, but I wouldn't count on it. Unless Uwe decides to do it. A 4.\", shape=(), dtype=string)\n",
            "Label neg\n",
            "Vectorized review tf.Tensor(\n",
            "[2347    1  284    7  122   70    2 4266 3387    1   12   13    2   83\n",
            "   28   18  177   38  646 7669    7  122   70  108 6333 1225  607  607\n",
            "  141  108    4  112  221 2431   16   11   28   29   40   12  396  492\n",
            " 4966    5  853   91  204  536    4   80 1244    8   54 1735 4928  437\n",
            "   13 2757   18   29 2141   23 1005 1878    2   83    1   13  424  591\n",
            "   50   70  228  320 4266 3387  208    6   12    3    1  284    1    2\n",
            "  745 1123   13   47    2  931 4683   65   47    2  594   13   47   18\n",
            "   11    7    9    2  224   53  147 5101   13  122   70    2   83   28\n",
            "   18    8    2  224    1 2240  418  100  377  216  797 1713    3   33\n",
            "  270   12 3551   20    3   20 4102    4  662    5    1   60   59   96\n",
            "    2    1   52   52 2780   18    9 2815   74  421  205    1  284  378\n",
            "    1    1  420 4590    1   14   33 2200 1031  288   15    4 4469    6\n",
            "  323   14   22  194  470    9   30  261    6 5307    4 4105 1199 7418\n",
            "  180    6   26    2 2049   15    2  853   41    1    1    3   14   22\n",
            "  194  507    4  590    5    1    3  104  853 2970  344    6  586    2\n",
            "  182   32  168   47    3    1    2  429   64  275 3308   57    9 2677\n",
            " 4569 5307   56  291   34   25    6   75    4    1    5  853  528   37\n",
            "   34   68  916    4 4469    6    2 2380 3391  188   11    3], shape=(250,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"128 ---> {vectorize_layer.get_vocabulary()[128]} \")\n",
        "print(f\"1 ---> {vectorize_layer.get_vocabulary()[1]} \")\n",
        "print(f\"4 ---> {vectorize_layer.get_vocabulary()[4]} \")\n",
        "print(f\"5563 ---> {vectorize_layer.get_vocabulary()[5563]} \")\n",
        "print(f\"313 ---> {vectorize_layer.get_vocabulary()[313]} \")\n",
        "print(f\"rozmiar słownika ---> {len(vectorize_layer.get_vocabulary())} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao-SVGop7KJe",
        "outputId": "1add2a91-04b0-4057-a7ca-8561678c4203"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128 ---> here \n",
            "1 ---> [UNK] \n",
            "4 ---> a \n",
            "5563 ---> underworld \n",
            "313 ---> night \n",
            "rozmiar słownika ---> 10000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text,label):\n",
        "    text = tf.expand_dims(text,-1)\n",
        "    return vectorize_layer(text),label"
      ],
      "metadata": {
        "id": "VdBGDqOO-3be"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "l13_P7Um8a2c"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "bKU8AXMg_Wuv"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}